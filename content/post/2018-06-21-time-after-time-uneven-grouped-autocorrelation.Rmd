---
title: 'Time after time: autocorrelation for uneven and/or grouped time series'
author: Ariel Muldoon
date: '2018-06-21'
slug: uneven-group-autocorrelation
categories:
  - r
  - statistics
tags:
  - autocorrelation
  - tidyr
draft: TRUE
description: "Checking for autocorrelation when some observations are missing from a time series and/or involves a time series in independent groups takes careful thought.  I show an approach to pad a dataset with NA via tidyr::complete() to fill in any missed sampling times while treating groups as independent."
---

Calculating autocorrelation when sampling was uneven and/or we have separate time series for different independent sample units takes careful thought.  It's easy to mistakenly ignore the issues, which can lead to a poor estimate of what kind and how much autocorrelation there might be.  

The situation I'm addressing in this post involves samples where the time is evenly spaced like, e.g., an annual or daily time series.  The unevenness I refer to is due to some samples being missing from some of the sampling bouts.  This is different than treating time as continuous, where we would take a different approach.

I first ran into this problem myself when I was analyzing data from a [rotating panel study design](http://methods.sagepub.com/reference/encyclopedia-of-survey-research-methods/n500.xml).  In the data I was working with some units were sampled every year, some sampled every 3rd year, some every 9th year, etc.  The sample units were considered independent replicates of each time series.  

It took me some time to figure out how to check for residual autocorrelation from models I fit,Since then I've periodically worked with students who needed to check for autocorrelation in the presence of unevenness and grouped time series.

In my working example today I'll use data data that has a pattern to the unevenness within groups, but the same approach applies to when some sampling events are missing because of unplanned events.  In addition, I'll focus on check for *residual* autocorrelation since that is what students I work with are most often doing (checking for any *leftover* autocorrelation, as things in the model could account for some autocorelation).

# Simulating data with autocorrelated noise

Autocorrelated noise can be simulated in R using the `arima.sim()` function.  [This link](https://stat.ethz.ch/pipermail/r-help/2008-July/168487.html) helped me figure out how to do this.  I'm working with the default distribution, which is the normal distribution with a mean 0 and standard devation 1.

I'll use helpers from **purrr** to do the looping and **dplyr** for any data manipulation.

```{r}
library(purrr) # v. 0.2.5
suppressPackageStartupMessages( library(dplyr) ) # v. 0.7.5
```

I'll start by setting the seed prior to doing the simulation.  I mix things up by using a seed of `64` instead of my go-to seed, `16`. `r emo::ji("laughing")`

```{r}
set.seed(64)
```

I'm going to start by simulating a 10-observation time series for 9 different sample units (`unit`).  Because each sample unit is an independent time series I do a loop via `map()` to simulate the 9 separate datasets.  

I simulate observations of the response variable `y` and explanatory variable `x` for each time series and index `time` with an integer to represent the time of the observations.  This time variable could be something like year or days or months or even depth in a soil core.  The key is that the unit of time is evenly spaced.

The response variable `y` is constructed based on a relationship with the explanatory variable `x` along with AR1 errors from the normal distribuiton.  I set the lag-1 correlation to 0.75.  The "lag-1" correlation is the correlation between the set of observed values observed at time $t$ with the values at $t-1$.  So in this example for one sample unit the lag-1 correlation is the observed values at sample times 2-10 with those at sample times 1-9.  The lag-2 correlation would be between the observations two sampling times apart (3-10 vs 1-8), etc.  With 10 observations the largest lag is a lag-9.  

If you run this code you will get warnings about losing the time series attribute from `arima.sim()`, which I've suppressed in the output below but aren't a problem for what I'm doing here.

```{r, warning = FALSE}
dat = map_dfr(1:9, 
               ~tibble(unit = .x,
                     x = runif(10, 5, 10),
                     y = 1 + x + arima.sim(list(ar = .7), 10),
                     time = 1:10)
)

head(dat, 15)
```

The list above results in 9 datasets that is all evenly spaced in time.  I want to have three units with samples taken only every three sampling times and three taken only ever 9 sampling times.  The last three will have been measured every time.

```{r}
dat = dat %>%
     filter(unit %in% 4:9 | time %in% c(1, 4, 7, 10)) %>%
     filter(!unit %in% 4:6 | time %in% c(1, 10) )

head(dat, 15)
```

# Fit model and extract residuals

I'm first going to show the results of a naive analysis where we ignore groups.  I'll fit the model of `y` vs `x` via the `lm()` function.

```{r}
fit1 = lm(y ~ x, data = dat)
```

I am interested in *residual* autocorrelation, so I will be working with the residuals from this model.  I add the residuals to the dataset to keep things organized.    

```{r}
dat$res = residuals(fit1)
```

# Problems with naively using acf() on the residuals

If we hadn't thought about our spacing issue in our grouped dataset, the next step would be to use the `acf()` function to check for any residual autocorrelation at various time lags.  The `acf()` function calculates and plots the autocorrelation function of a vector of values (see `pacf()` for partial autocorrelation).  That sounds like what we want; so why do I say this would be naive in this example?  

The `acf()` function expects values to be in order by time and assumes equal spacing in time.  If we calculate the autocorrelation function directly on the residuals as they are now, we are ignoring instances where we didn't sample for several years so any two obserations that are next to each other are considered 1 lag apart even though it may have been three years since the last sample.  In addition, the `acf()` function doesn't know we have groups.  The last observations of one time series comes immediately before the first observation of another, and are considered one lag apart even though they are unrelated. 

The `acf()` function is expecting the dataset to be in order by time, so make sure things are in order prior to using `acf()`.  Since I'm working with grouped data I'll make sure things are in order by time within each group.

One option for data like this, where we are assuming normally distributed errors, is to work with the **nlme** package.  That package ahs an `ACF()` function that works on both `gls` and `lme` objects.  However, there are plenty of models that can't be fit with those functions, so I'll show a more general appraoch.

We need to make sure that the dataset has the appropriate spacing.  This means we need to add rows so that all times have an appropriate number of rows between them.  In addition, we need to add rows between groups so we don't mistakenly treat them as if groups were not independent.
# Figure out maximum lag of interest via group sizes

# Deciding on a maximum lag to calculate based on group size

In this example, the units are considered to be independent of each other.  So when checking for autocorrelation, the maximum lag to check for will be based on the maximum group size.  Here's one way to check for that.  Because I simulated these data I already know that the longest time series is 10, but when working with real data this sort of check would be standard.

```{r}
dat %>%
    count(unit)
```

That wouldn't be useful for a large number of groups, though, so here's an alternative to look at just the maximum group size.

```{r}
dat %>%
    count(unit) %>%
    filter(n == max(n) )
```

# Pad the dataset with tidyr::complete()

When I was thinking through this problem for the first time, I found a [these lecture notes](http://www.unc.edu/courses/2010spring/ecol/562/001/docs/lectures/lecture10.htm#testing) that showed the basic idea of padding the residuals to get the spacing between groups correct.  I went a slightly different route at that point, where I did some joining to get the proper spacing.  Since then I've found the `complete()` function from package **tidyr** to be one of the more straightforward coding approaches for students.

```{r}
library(tidyr) # v. 0.8.1
```

A variable representing the autocorrelation variable is needed for this approach.  In this case it's `time`, but if I was doing soil depth that was taken evenly through a soil core it could be something like soil depth.

I group the dataset so every group is padded with rows of missing values at the end.

```{r}
dat_expand = dat %>%
    group_by(unit) %>%
    complete(time = 1:20) 
```

Here is an example of the first group, which has rows added for the times that weren't sampled along with 10 rows of `NA` at the end.  

```{r}
filter(dat_expand, unit == 1)
```

# Calculate residual autocorrelation of properly-spaced residuals

Now calculate the residual autocorrelation again using the expanded dataset, using `na.action = na.pass`.  Now groups are not mistakenly considered in the autocorrelation since rows of observations are no longer contiguous.

```{r}
acf(dat_expand$res, lag.max = 9, na.action = na.pass, ci = 0)
```

# The confidence interval on the ACF plot

I removed the confidence interval from the plot above.  The calculated confidence interval counts the NA values as part of the sample size, so the confidence interval is too narrow.  You can calculate your own confidence interval based on the actual sample size, but you have to calculate the true sample size for each lag yourself.  I refer you again to the [lecture notes](http://www.unc.edu/courses/2010spring/ecol/562/001/docs/lectures/lecture10.htm#acf) for some ideas.  I'll put code I've used before below as an example, but I won't walk through it.

```{r}
( nall = map_df(1:9, ~
    dat %>%
        group_by(unit) %>%
        arrange(unit, time) %>%
        summarise(diff = list( diff(time, lag = .x )  ) )
     ) %>%
       unnest(diff) %>%
       group_by(diff) %>%
       summarise(n = n() ) )
```


And here's the ACF plot with 95% CI added via `lines()`.

```{r}
acf(dat_expand$res, lag.max = 9, na.action = na.pass, ci = 0)
lines(1:9,-qnorm(1-.025)/sqrt(nall$n), lty = 2)
lines(1:9, qnorm(1-.025)/sqrt(nall$n), lty = 2)
```

