---
title: Combining many datasets in R
author: Ariel Muldoon
date: '2017-12-31'
slug: many-datasets
draft: TRUE
categories:
  - r
tags:
  - data
  - teaching
---

At least once a year I meet with a graduate student who has many datasets that need to be combined prior to any data exploration or analysis.  The data are usually from some sort of data logger (e.g, iButtons or RFID readers) that records data remotely over some time period, which the researcher then downloads.  As data loggers become less expensive and have more memory and/or better batteries, I believe such scenarios will only be getting more common.  

Today I wanted to go through some code on how to do this sort of task in R.  I'm first going to set up the background for my particular use case.  **Skip straight to "List all files" if you want to jump right into code.**

# What's so hard about reading in many datasets?

For someone who is at least somewhat familiar with a programming language (e.g., SAS, Python, R), reading many datasets in and combining them into a single file might not seem like a big deal.  For example, if I do a quick web search on something like `"r read many datasets"` I get at least 5 Stack Overflow posts as well as a few blog entries.  These links show code for relatively simple situations of reading many identical dataset in to R with `lapply` or a loop (a couple SO examples [here](https://stackoverflow.com/questions/17271833/how-do-i-read-in-multiple-data-sets) and [here](https://stackoverflow.com/questions/32888757/reading-multiple-files-into-r-best-practice)).

But, frankly, in my experience this doesn't feel very simple to beginner programmers.  Most of the graduate students I meet with have never worked with any sort of programming language prior to entering their degree program.  By the time I meet with them they usually have had a very basic introduction to R in their basic statistics courses.  They may have read in a dataset into R only a couple of times, and now they have hundreds of them to manage.  

*(Aside:  I am couching this around R because that's what taught in the intro courses in the Statisics Department at my university; I still get a few requests every year for help with SAS programming from faculty and post-docs and so far over six years I've had exactly one student client who worked primarily with Python)*

# Why would I want to have to do this in R (or SAS or Python or ...)?

That is the question I got from the first student I ever advised on this process.  She was collecting data using many data loggers in a field experiment set up as randomized complete block design with repeated measures.  She had 300 comma-delimited files that needed to be concatenated together from her first field season.  She would be doing a second season, which would have at least twice as many files.

Her research collaborators had used these loggers previously, and had given her the following algorithm to follow:

1. Open each file in Excel
2. Manually delete the first 15 rows, which contained information about the data logger that wasn't related to the study
3. Add columns to indicate the physical units the data was collected from (Block, Site, Plot)
4. Copy and paste into a new file
5. Repeat with all datasets
6. Name columns

Me:
```{r, echo = FALSE}
knitr::include_graphics("/img/2017-12-31_cat_disgusted.png")
```

But really this gave me a chance to discuss reproducibility and the convenience of using computers to do repetitive tasks.  Some effort in the short term can be very valuable in the long term.

To be honest, she was pretty skeptical that it made sense to use a programming language to do the work.  It helped that we found mistakes in the files she'd already edited when when we were setting up the R code (it's sooo easy to make mistakes when copying and pasting 300 times!).  Her skepticism continues to be a good reminder to me of what it feels like to be a beginner in a programming language, where you never quite trust that the program is doing what you want it to and so the manual approach you already know how to do looks pretty darn attractive.

# List all files to read in

When reading in many files, the first two tasks are to:
+ Getting a list of files to read
+ Figuring out the steps needed to read and manipulate a single file

Here I'll start by getting a list of files.  I'm using some toy files I made, which are CSV files. These are available on the [blog GitHub repository](https://github.com/aosmith16/aosmith).

This can be done in R via `list.files` or `dir`.  I'll use `list.files` here out of habit.  In this particular case I will need four of the arguments.

1. The directory containing the files needs to be defined in the `"path"` argument of `list.files`.  I'm working within an RStudio Project, and will use the `here` package to indicate the directory the files are in.  See [Jenny Bryan's post here](https://www.tidyverse.org/articles/1/01/) on the merits of `here` and self-contained projects.

2. The `"pattern"` argument is where we can indicate which files we want to read in.  If you want to read in all CSV files in a directory, the pattern to match might by `".csv"`.  In the real scenario, there were additional CSV files in the directory that shouldn't be read, so we first defined the the pattern as `"AB.csv"` and then, after realizing that some file names were all lowercase, as `"AB.csv|ab.csv"`.  (The vertical pipe, `|`, stands for "or").

3. The `"recursive"` argument is used to indicate that files within child folders of the defined directory should also be read.  The files in this particular case are not stored in a single folder.  Instead they are in folders within the overall directory.  The child folders indicate the study units (Blocks and Sites nested in Blocks) the data were collected in. 

4. The `"full.names"` argument is if the full file names should be returned or only the relative file path.  In this case, the only place in the information about the physical units of the study ("Blocks" and "Sites") are in the directory path. We needed the full path names in order to extract that information and add it to the dataset in R.

```{r, message = FALSE}
library(here) # v. 0.1
```

Here are the files for this example.  The result is a vector of files to read in.

```{r}
( allfiles = list.files(path = here("content", "data"),
                        pattern = "AB.csv|ab.csv",
                        full.names = TRUE,
                        recursive = TRUE) )
```

# Practice reading in one file

I find things tend to go more smoothly if I work out the file-reading process with a single file before I try to read a bunch of files in.  It's an easy step to want to skip because it feels more efficient to do "everything at once".  I've never found that to actually be the case.

Here I'll read in the first file in the files list.  The top 6 lines is all extraneous header information, which should be skipped.  There are no column headers in the file, so those need to be added.

```{r}
( test = read.csv(allfiles[1], 
                skip = 6,
                header = FALSE,
                col.names = c("date", "temperature") ) )
```

That went pretty smoothly, but things get a little hairy from here.  The information on the physical units of the study, "Blocks" and "Sites", are contained only in the file directory path.  These need to be extracted from the file path and added to the dataset.  

In addition, the "Plot" information is contained in the file name. Plot names are single numbers that are found directly before the period in the file name.  In `allfiles[1]`, `r allfiles[1]`, that number is 5.

Last, the last two digits of the file name is code to indicate where the data logger was located, which also needs to be added to the dataset.  In this case these values are all "AB", but in the larger set of files this wasn't true.

All the tasks above are string manipulation tasks.  I will tackle these with the functions from the **stringr** package.

```{r}
library(stringr) # v. 1.2.0
```


**Extract "Block" names from the file path**

Splitting the file path into pieces is a reasonable first step.  This can be done via `str_split` using `"/"` as the symbol to split on.  Because there is only a single character string we want to split for each dataset, it is convenient to return a matrix instead of a list via `simplify = TRUE`.

The result is a matrix containing strings in each column.

```{r}
( allnames = str_split( allfiles[1], pattern = "/", simplify = TRUE) )
```


The information on blocks is always the third column if counting from the end instead of the beginning.  It's "safer" to go from the last column backwards here, as on a different computer the full file paths may be different.

To get the third from the end I take the total number of columns and subtract 2.

```{r}
allnames[, ncol(allnames) - 2]
```

So this could be added to the dataset as a "block" variable.

```{r}
test$block = allnames[, ncol(allnames) - 2]
test
```

**Extract "Site" names from the file path**

This will be the same as above, except site names are contained in the second-to-last column.

```{r}
test$site = allnames[, ncol(allnames) - 1]
```

**Extract "Plot" names from the file name**

The last character string in our matrix is the file name, which contains the plot name and logger location.  In the test case the plot name is "5" and the logger location is "AB".

```{r}
allnames[, ncol(allnames)]
```

This can be split on the underscores and periods and the plot information extracted in much the same was as the "Block" information.  I think this option can feel more approachable to beginners and is a reasonable way to approach the problem.  

Another option is to use "regular expressions" to define which part of the file name we want to pull out.  I continually find regular expressions difficult, but I will use them here to give a wider breadth of examples.  

A basic introduction to regular expressions is on the second page of the "Work with Strings" [cheatsheet from RStudio](https://www.rstudio.com/resources/cheatsheets/).  More in depth examples are on the [UBC ST 545 page](http://stat545.com/block022_regular-expression.html).

We can extract the desired number from the file name via `str_extract` from **stringr**, using regular expressions for the `"pattern"`.  In this case I used a [lookaround](http://www.regular-expressions.info/lookaround.html) following this [Stack Overflow answer](https://stackoverflow.com/a/35404422/2461552).  These can be apparently be costly in terms of performance, which I did not find to be a deterrent in my case.

The regular expression I use extracts a digit (`[0-9]`) that comes just before a period.  The `(?=\\.)` is a *positive lookahead*, telling R to look for a period following the digit we want to extract. The plot names are always just in front of a period, which is why this works.

```{r}
str_extract(allnames[, ncol(allnames)], pattern = "[0-9](?=\\.)")
```

This can then be added to the dataset.

```{r}
test$plot = str_extract(allnames[, ncol(allnames)], pattern = "[0-9](?=\\.)")
```

**Extract data logger location from the file name**

The last thing to do is extract the data logger location code from the file names.  These are the last two digits of the file name, immediately in front of ".csv".

The `str_sub` function from **stringr** is useful for extracting characters from fixed locations in a string.  The logger location information is in the same position in every file name if counting from the end of the string.  The `str_sub` function allows us to pull information counting from the end of the string instead of only the beginning.  Because our file names differ in length (due to the way that dates are stored), the location data is *not* does not always have the same indices if counting characters from the front of the string.

Negative indices are used to extract from the end of the string.  The information is in positions 5 and 6.

```{r}
str_sub(allnames[, ncol(allnames)], start = -6, end = -5)
```

This also should be added to the dataset as it is read in.  Remember that at least one of the file names is all lowercase, so we can make sure the data logger location information is in all caps via `toupper`.

```{r}
test$logloc = toupper( str_sub(allnames[, ncol(allnames)], start = -6, end = -5) )
```

Here's what our test dataset looks like now.

```{r}
test
```

# Make a function to read all the files

Once the process is worked out, we can "functionize" it (aka, make a function) so we can do the same procedure to every dataset that needs to be read in and combined.  

This function takes a single argument, which is the file path of the dataset.  This takes all the work we did on a single file and generalizes it to work with all the file paths we have, returning the modified dataset.

```{r}
read_fun = function(path) {
     test = read.csv(path, 
                skip = 6,
                header = FALSE,
                col.names = c("date", "temperature") )
     allnames = str_split( path, pattern = "/", simplify = TRUE)
     test$block = allnames[, ncol(allnames) - 2]
     test$site = allnames[, ncol(allnames) - 1]
     test$plot = str_extract(allnames[, ncol(allnames)], pattern = "[0-9](?=\\.)")
     test$logloc = toupper( str_sub(allnames[, ncol(allnames)], start = -6, end = -5) )
     test
}
```

I'll test the function on that first file path again to make sure it works as I expect it to.  

```{r}
read_fun(allfiles[1])
```

Looks good!

# Read all the files

All that's left to do is to loop through all the file paths in `allfiles`, read and modify each one, and combine them into a single dataset.  This can be done in base R with a `for` loop or an `lapply` loop.  If using those options, the final concatenation step would be to use `rbind` in `do.call`.  

These days I've been using the `map` functions from package **purrr* for this, mostly because the `map_dfr` variant conveniently bind everything together by rows.

```{r}
library(purrr) # v. 0.2.3
```

Here's what using `map_dfr`, looping through each element of `allfiles` to read and modify the datasets with the `read_fun` function and then binding everything together into a final combined dataset.

```{r}
( combined_dat = map_dfr(allfiles, read_fun) )
```

# Are we finally done?

Hopefully! `r emo::ji("laugh")`

In working with real data, the final "combining" step can lead to errors due to unanticipated complexities.  In my experience, this most often happens because some of the datasets are physically different than the rest.  I've worked on problems where, for example, it turned out some datasets were present in the directory but were empty.

In the real files for this particular example, it turned out some of the files had been previously modified manually to remove the header information. We ended up adding an `if` statement to the function to test the file as we read it in.  If it had the header information we'd use `skip` and if it didn't we wouldn't.  I did something similar (i.e., used an `if` statement) in the case where some of the datasets were blank.

Once everything is worked out and the combined dataset has been created, you might want to save it for further data exploration and/or analysis.  For students who are working on an interim set of datasets (before the end of their field season), saving the R object with `saveRDS` can be pretty convenient.  Saving a final dataset as a CSV may be useful for sharing with collaborators once all datasets have been downloaded and combined, though, which can be done with, e.g., `write.csv`.  
