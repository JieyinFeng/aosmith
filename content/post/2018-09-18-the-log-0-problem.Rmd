---
title: 'The log-0 problem: analysis options and choosing c in log(y + c) '
author: Ariel Muldoon
date: '2018-09-18'
slug: the-log-0-problem
categories:
  - r
  - statistics
tags:
  - teaching
  - simulation
draft: TRUE
description: "Analyzing right-skewed continuous data with standard statisical tools can be challenging when the data also contains 0 values.  I discuss the thought process I go through when I'm in this situation to decide on an analysis strategy.  I go on to explore the effect of the constant, c, when using log(y + c) as a response variable"
---



# [Choosing c for log(y + c)](#choosing-c-for-log-y-c)

It may be that after all that hard thinking you end up on the option of adding a constant to shift your distributon away from 0.  In this case, what should this constant, `c`, be?

This choice isn't minor, as the value you choose can change your results.  The more 0 values you have the more likely the choice of `c` matters.

Some options:

- Add 1.  I think folks find this attractive because log(1) = 0.  However, whether or not 1 is reasonable can depend on the distribution of your data. 

- Add half the minimum non-0 value.  This is what I was taught to do in my statistics program, and I've heard of others with the same experience.  As far as I know this is an unpublished recommendation.

- Add the quadrat of the first quartile divided by the third quartile.  This recommendation reportedly comes from [Stahel 2008](https://stat.ethz.ch/~stahel/stat-dat-ana/); I have never verified this since I unfortunately don't know German.  This approach clearly only is relevant if the first quartile is greater than 0, so you must have fewer than 25% 0 values to use this option.

# Load R packages

I'm ready to start exploring these options, so I'll look the packages I need today.

```{r, message = FALSE, warning = FALSE}
library(purrr) # v. 0.2.5
library(dplyr) # v. 0.7.6
library(broom) # v. 0.5.0
library(ggplot2) # v. 3.0.0
library(ggridges) # v. 0.5.0
```

# Generate log-normal data with 0 values

Since the log-normal distribution by definition has no 0 values, it's not that easy to simulate such data.  When I was working on this several years ago I decided I could force 0 values in through rounding.  This is not the most elegant solution, I'm sure, but I think it works to show how the value of `c` can change results.

I achieve this by generating negative data on the log scale, and then exponentiating to the data scale.  The range of the `y` variable ends up with many small values but can have some fairly large values, as well.

The data is negative because my `x` variable is negative and is positively related to `y`.  

I'll set the seed and then set the values of the parameters.  I'm using a sample size of 50.

```{r}
set.seed(16)
beta0 = 0 # intercept
beta1 = .75 # slope

# Sample size
n = 50

# Explanatory variable
x = runif(n, min = -7, max = 0)
```

The response variable is calculated from the parameters and the random errors.  Those results are exponetiated to get the data to be used for a log transformation.

```{r}
true_y = exp(beta0 + beta1*x + rnorm(n, 0, 2))
summary(true_y)
```

I force 0 values into the dataset by rounding `true_y` to two decimal places.

```{r}
y = round(true_y, digits = 2)
summary(y)
```

I fooled around a lot with the parameters, `x`, and the residual errors to get 0's in the rounded `y` most of the time.

```{r}
replicate(100, sum(round(exp(beta0 + beta1*runif(n, -7, 0) + rnorm(n, 0, 2)), 2) == 0) )
```

You can see there is variation in the estimated slope, depending on the option.

Here's the "true" model, fit to the unrounded data.

```{r}
( true = lm(log(true_y) ~ x) )
```

The log(y + 1) model fit to the rounded data.

```{r}
( fit1 = lm(log(y + 1) ~ x) )
```

The colloquial method of half the minimum non-0 value I was taught.

```{r}
( fitc = lm(log(y + min(y[y>0])/2) ~ x) )
```

And the quartile method per Stahel 2002.

```{r}
( fitq = lm(log(y + quantile(y, .25)^2/quantile(y, .75) ) ~ x) )
```

# A function for fitting models

I decided I want to fit the same models to the same data, so made the function accordingly.  I'm going to fit all four models above within each call to the function.    

The `beta0` and `beta1` arguments can technically be changed, but I'm pretty tied into the values I chose for them and for `x` since I had a difficult time getting enough (but not too many!) 0 values.

I wanted to make sure that I always had at least one 0 in the rounded `y` data but that less than 25% of the values were 0 so I included a `while()` loop.

The function returns a list of models.  I give each model in the output list a name for organization later.

```{r}
logy_0 = function(beta0 = 0, beta1 = .75, n) {
     x = runif(n, -7, 0) # create expl var between -10 and 0
     true_y = exp(beta0 + beta1*x + rnorm(n, 0, 2))
     y = round(true_y, 2)
     
     while( sum(y == 0 ) == 0 | sum(y == 0) > n/4) {
          true_y = exp(beta0 + beta1*x + rnorm(n, 0, 2))
          y = round(true_y, 2)
     }
     
     true = lm(log(true_y) ~ x)
     fit1 = lm(log(y + 1) ~ x)
     fitc = lm(log(y + min(y[y>0])/2) ~ x)
     fitq = lm(log(y + quantile(y, .25)^2/quantile(y, .75) ) ~ x)
     
     setNames(list(true, fit1, fitc, fitq), 
              c("True model", "Add 1", "Add 1/2 minimum", "Quartile method") )
}
```

Do I get the same values back as my manual work if I reset the seed to 16?  Yes!  `r emo::ji("raised_hands")`

```{r}
set.seed(16)
logy_0(n = 50)
```

# Extract the results

First I'll generate data and fit the models many times.  I'm doing this 1000 times today.  This is a list of lists.

```{r}
models = replicate(1000, logy_0(n = 50), simplify = FALSE)
```

Then I can loop through the models and extract the output of interest.  Today I'm interested in the estimated coefficients, with confidence intervals.  I can extract this information using `broom::tidy()` for `lm` objects.

I use `flatten()` to turn `models` into a single big list instead of a list of lists.  I loop via `map_dfr()`, so my result is a data.frame that contains the coefficients as well as the description of each model.    

```{r}
results = map_dfr(flatten(models), 
              ~tidy(.x, conf.int = TRUE), 
              .id = "model")
head(results)
```

I'm going to focus on only the slopes, so will pull those out.

```{r}
results_sl = filter(results, term == "x")
```

# Compare the options for c

So how does changing the values of c change the results?  Does it matter what we add?

## Summary stastics

First I'll calculate a few summary statistics.  Here is the median estimate of the slope for each "c" option and the confidence interval coverage (i.e., the proportion of times the confidence interval contained the true value of the slope; for a 95% confidence interval this should be 0.95).

```{r}
results_sl %>%
     group_by(model) %>%
     summarise(med_estimate = median(estimate),
               CI_coverage = mean(conf.low < .75 & .75 < conf.high) )
```

You can see right away that, as we'd expect, the actual model fit to the unrounded data does a good job. The confidence interval coverage is close to 0.95 and the median estimate is right at 0.75 (the true value).  

The log(y + 1) models performed extremely poorly.  The estimated slope is biased extremely low.  None of the 1000 models ever had the true slope in the confidence interval.

The other two options performed better.  The quartile method gives estimates that are, on average, too high.  The "add 1/2 minimum" model underestimates the slope.  The confidence interval coverage is not good, but I suspect this has at least partially to do with the reduction in variance from the rounding that I did.  

## Graph the results

Here is a graph, using package **ggridges** to make ridge plots.  This makes it easy to compare the distribution of slope estimates from each option to the true model (shown at the top in yellow).

Again, the estimates from the "Add 1" models are clearly not useful here.  The slope estimate are all biased incredibly low.

The "Quartile method" has the widest distribution.  It is long-tailed to the right compared to the distribution of slopes from the true model, which is why it can overestimate the slope, on average.  

And the "Add 1/2 minimum" option tends to underestimate the slope.  I'm curious that the distribution is also relatively narrow.

```{r, message = FALSE}
ggplot(results_sl, aes(x = estimate, y = model, fill = model) ) +
     geom_density_ridges2(show.legend = FALSE, rel_min_height = 0.005) +
     geom_vline(xintercept = .75, size = 1) +
     scale_fill_viridis_d() +
     theme_bw() +
     scale_x_continuous(name = "Estimated slope", expand = c(0.01, 0)) +
     scale_y_discrete(name = NULL, expand = expand_scale(mult = c(.01, .3) ) )
```

# What option to use?

Mostly what I found out here is that my method for simulating positive, continuous data plus 0 values seems kludgy.  For the data I did simulate, though, adding 1 is clearly a very bad option if you want to get a reasonable estimate (which is generally my goal when I'm doing an analysis! `r emo::ji("laugh")`.)

The other two options performed OK in this specific scenario I created.  It'd be nice to understand if the number of 0 values in the distribution has an impact on the results, because it seems like that could influence the choice between the two options.

Deciding whether you want to over- or under-estimate (on average) is difficult.  Statisticians generally loathe anti-conservative methods, so my guess is statisticans would choose the conservative option ("Add 1/2 minimum") to be on the safe side.  In reality, if we saw these same results in other simulated scenarios the choice would need to be based on how bad a mistake it would be to potentially miss the relationship vs how bad it would be to overstate the relationship.
