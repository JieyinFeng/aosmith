---
title: Unstandardizing coefficients from a GLMM
author: Ariel Muldoon
date: '2018-03-26'
slug: unstandardizing-coefficients
categories:
  - r
  - statistics
tags:
  - glmm
  - lme4
draft: TRUE
description: "Unstandardizing coefficients in order to interpret them on the original scale is necessary when explanatory variables were standardized to help with convergence when fitting a generalized linear mixed model.  Here I show one way to do this for a generalized linear mixed model fit with lme4." 
---



<p>Winter term grades are in and I can now once again scrape together some time to write blog posts! üéâ</p>
<p>The last post I did about making <a href="https://aosmith.rbind.io/2018/01/31/added-variable-plots/">added variable plots</a> led me to think other topics involving getting model results, including the one I‚Äôm talking about today: unstandardizing coefficients.</p>
<p>This comes up particularly for generalized linear mixed models (GLMM), where models don‚Äôt always converge if explanatory variables are left unstandardized. The lack of convergence can be caused by explanatory variables with very different magnitudes, and so standardizing the variables prior to model fitting can be useful. In such cases, coefficients and confidence interval limits usually need to be converted to their unstandardized values for interpretation.</p>
<p>The math for converting the standardized slope estimates to unstandardized ones turns out to be fairly straightforward. Coefficients for each variable need to be divided by the standard deviation of that variable (this is only true for slopes, not intercepts). The math is shown <a href="https://stats.stackexchange.com/questions/74622/converting-standardized-betas-back-to-original-variables">here</a>.</p>
<p>The first time I went though this process was pretty clunky. I‚Äôve managed to tidy it up quite a bit through some recent work with students, which I‚Äôm demonstrating today.</p>
<div id="r-packages" class="section level1">
<h1>R packages</h1>
<p>Model fitting will be done via <strong>lme4</strong>, which is where I‚Äôve most often needed to do this. Data manipulation tools from <strong>dplyr</strong> will be useful for getting results tidied up. I‚Äôll also use helper functions from <strong>purrr</strong> to loop through variables and <strong>broom</strong> for the tidy extraction of fixed-effects coefficients from the model.</p>
<pre class="r"><code>library(lme4) # v. 1.1-15</code></pre>
<pre><code>## Loading required package: Matrix</code></pre>
<pre class="r"><code>suppressPackageStartupMessages( library(dplyr) ) # v. 0.7.4
library(purrr) # 0.2.4
library(broom) # 0.4.3</code></pre>
</div>
<div id="the-dataset" class="section level1">
<h1>The dataset</h1>
<p>The dataset I‚Äôll use is named <code>cbpp</code>, and comes with <strong>lme4</strong>. It is a dataset of counted proportions, so is analyzed via a binomial generalized linear mixed model.</p>
<pre class="r"><code>glimpse(cbpp)</code></pre>
<pre><code>## Observations: 56
## Variables: 4
## $ herd      &lt;fct&gt; 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5...
## $ incidence &lt;dbl&gt; 2, 3, 4, 0, 3, 1, 1, 8, 2, 0, 2, 2, 0, 2, 0, 5, 0, 0...
## $ size      &lt;dbl&gt; 14, 12, 9, 5, 22, 18, 21, 22, 16, 16, 20, 10, 10, 9,...
## $ period    &lt;fct&gt; 1, 2, 3, 4, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3...</code></pre>
<p>This dataset has no continuous explanatory variables in it, so I‚Äôll add some. I decided to create three new variables with very different ranges.</p>
<pre class="r"><code>set.seed(16)

cbpp = mutate(cbpp, y1 = rnorm(56, 500, 100),
              y2 = runif(56, 0, 1),
              y3 = runif(56, 10000, 20000) )

glimpse(cbpp)</code></pre>
<pre><code>## Observations: 56
## Variables: 7
## $ herd      &lt;fct&gt; 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5...
## $ incidence &lt;dbl&gt; 2, 3, 4, 0, 3, 1, 1, 8, 2, 0, 2, 2, 0, 2, 0, 5, 0, 0...
## $ size      &lt;dbl&gt; 14, 12, 9, 5, 22, 18, 21, 22, 16, 16, 20, 10, 10, 9,...
## $ period    &lt;fct&gt; 1, 2, 3, 4, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3...
## $ y1        &lt;dbl&gt; 547.6413, 487.4620, 609.6216, 355.5771, 614.7829, 45...
## $ y2        &lt;dbl&gt; 0.87758754, 0.33596115, 0.11495285, 0.26466003, 0.99...
## $ y3        &lt;dbl&gt; 14481.07, 11367.88, 14405.16, 18497.73, 17955.66, 10...</code></pre>
</div>
<div id="analysis" class="section level1">
<h1>Analysis</h1>
<div id="unstandardized-model" class="section level2">
<h2>Unstandardized model</h2>
<p>Here is my initial generalized linear mixed model, using the three continuous explanatory variables as fixed effects and ‚Äúherd‚Äù as the random effect. A warning message indicates that standardizing might be necessary.</p>
<pre class="r"><code>fit1 = glmer( cbind(incidence, size - incidence) ~ y1 + y2 + y3 + (1|herd),
              data = cbpp, family = binomial)</code></pre>
<pre><code>## Warning: Some predictor variables are on very different scales: consider
## rescaling</code></pre>
<pre><code>## Warning in checkConv(attr(opt, &quot;derivs&quot;), opt$par, ctrl = control
## $checkConv, : Model failed to converge with max|grad| = 0.947357 (tol =
## 0.001, component 1)</code></pre>
<pre><code>## Warning in checkConv(attr(opt, &quot;derivs&quot;), opt$par, ctrl = control$checkConv, : Model is nearly unidentifiable: very large eigenvalue
##  - Rescale variables?;Model is nearly unidentifiable: large eigenvalue ratio
##  - Rescale variables?</code></pre>
</div>
<div id="standardizing-the-variables" class="section level2">
<h2>Standardizing the variables</h2>
<p>I‚Äôll now standardize the three explanatory variables, which involves subtracting the mean and then dividing by the standard deviation. The <code>scale()</code> function is one way to do this in R.</p>
<p>I do the work inside <code>mutate_at()</code>, which allows me to choose the three variables I want to standardize by name and add ‚Äús‚Äù as the suffix by using a name in <code>funs()</code>. Adding the suffix allows me to keep the original variables, as I will need them later. The extra <code>as.numeric()</code> has to do with what the <code>scale()</code> function returns.</p>
<pre class="r"><code>cbpp = mutate_at(cbpp, vars( y1:y3 ), funs(s = as.numeric( scale(.) ) ) )

glimpse(cbpp)</code></pre>
<pre><code>## Observations: 56
## Variables: 10
## $ herd      &lt;fct&gt; 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5...
## $ incidence &lt;dbl&gt; 2, 3, 4, 0, 3, 1, 1, 8, 2, 0, 2, 2, 0, 2, 0, 5, 0, 0...
## $ size      &lt;dbl&gt; 14, 12, 9, 5, 22, 18, 21, 22, 16, 16, 20, 10, 10, 9,...
## $ period    &lt;fct&gt; 1, 2, 3, 4, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3...
## $ y1        &lt;dbl&gt; 547.6413, 487.4620, 609.6216, 355.5771, 614.7829, 45...
## $ y2        &lt;dbl&gt; 0.87758754, 0.33596115, 0.11495285, 0.26466003, 0.99...
## $ y3        &lt;dbl&gt; 14481.07, 11367.88, 14405.16, 18497.73, 17955.66, 10...
## $ y1_s      &lt;dbl&gt; 0.34007250, -0.32531152, 1.02536895, -1.78352139, 1....
## $ y2_s      &lt;dbl&gt; 1.4243017, -0.4105502, -1.1592536, -0.6520950, 1.837...
## $ y3_s      &lt;dbl&gt; -0.2865770, -1.3443309, -0.3123665, 1.0781427, 0.893...</code></pre>
</div>
<div id="standardized-model" class="section level2">
<h2>Standardized model</h2>
<p>The model with standardized variables converges without a problem.</p>
<pre class="r"><code>fit2 = glmer( cbind(incidence, size - incidence) ~ y1_s + y2_s + y3_s + (1|herd),
              data = cbpp, family = binomial)</code></pre>
</div>
</div>
<div id="unstandardizing-slope-coefficients" class="section level1">
<h1>Unstandardizing slope coefficients</h1>
<div id="get-coefficients-and-profile-confidence-intervals-from-model" class="section level2">
<h2>Get coefficients and profile confidence intervals from model</h2>
<p>If I were to use this model for inference I would want to unstandardize the coefficients before reporting them. The first step in the process is to get the standardized estimates and confidence intervals from the model; I use <code>tidy()</code> from package <strong>broom</strong> for this. The help page is at <code>?tidy.merMod</code> if you want to explore the options.</p>
<p>I chose to get profile likelihood confidence intervals as well as the estimates for the fixed effects only.</p>
<pre class="r"><code>coef_st = tidy(fit2, effects = &quot;fixed&quot;,
     conf.int = TRUE,
     conf.method = &quot;profile&quot;)</code></pre>
<pre><code>## Computing profile confidence intervals ...</code></pre>
<pre class="r"><code>coef_st</code></pre>
<pre><code>##          term   estimate std.error  statistic      p.value     conf.low
## 1 (Intercept) -2.0963512 0.2161353 -9.6992521 3.037166e-22 -2.565692724
## 2        y1_s  0.2640116 0.1395533  1.8918334 5.851318e-02 -0.009309633
## 3        y2_s  0.1031655 0.1294722  0.7968153 4.255583e-01 -0.151459144
## 4        y3_s  0.1569910 0.1229460  1.2769107 2.016338e-01 -0.086291865
##    conf.high
## 1 -1.6536063
## 2  0.5472589
## 3  0.3635675
## 4  0.4033177</code></pre>
</div>
<div id="calculate-standard-deviations-for-each-variable" class="section level2">
<h2>Calculate standard deviations for each variable</h2>
<p>I need the standard deviations for each variable in order to unstandardize the coefficients. If I do this right, I can get the standard deviations into a data.frame that I can then join to <code>coef_st</code>. Once I do that, dividing the estimated slopes and confidence interval limits by the standard deviation should be straightforward.</p>
<p>I will calculate the standard deviations per variable with <code>map()</code> from <strong>purrr</strong>, as it is a convenient way to loop through columns. I pull out the variables I want to calculate standard deviations for via <code>select()</code>. An alternative approach would have been to take the variables from columns and put them in rows (i.e., put the data in <em>long</em> format), and then summarize by groups.</p>
<p>The output from <code>map()</code> returns a list, which can be stacked into a long format data.frame via <code>utils::stack()</code>. This results in a two column data.frame, with a column for the standard deviation (called ‚Äúvalues‚Äù) and a column with the variable names.</p>
<pre class="r"><code>map( select(cbpp, y1:y3), sd) %&gt;% 
     stack()</code></pre>
<pre><code>##         values ind
## 1   90.4430192  y1
## 2    0.2951881  y2
## 3 2943.2098667  y3</code></pre>
<p>The variables in my model and in my output end with <code>_s</code>, so I need to add the suffix the variable names before I can join the two data.frames together.</p>
<pre class="r"><code>sd_all = map( select(cbpp, y1:y3), sd) %&gt;% 
     stack() %&gt;%
     mutate(ind = paste(ind, &quot;s&quot;, sep = &quot;_&quot;) )

sd_all</code></pre>
<pre><code>##         values  ind
## 1   90.4430192 y1_s
## 2    0.2951881 y2_s
## 3 2943.2098667 y3_s</code></pre>
</div>
<div id="joining-standard-deviations-to-the-coefficients-table" class="section level2">
<h2>Joining standard deviations to the coefficients table</h2>
<p>Now I can join the standard deviations to the coefficients data.frame. I‚Äôm not unstandardizing the intercept at this point, so I‚Äôll use <code>inner_join()</code> to only keep rows that have a match in both data.frames. Notice that the columns I‚Äôm joining by have different names in the two data.frames.</p>
<pre class="r"><code>coef_st %&gt;%
     inner_join(., sd_all, by = c(&quot;term&quot; = &quot;ind&quot;) )</code></pre>
<pre><code>##   term  estimate std.error statistic    p.value     conf.low conf.high
## 1 y1_s 0.2640116 0.1395533 1.8918334 0.05851318 -0.009309633 0.5472589
## 2 y2_s 0.1031655 0.1294722 0.7968153 0.42555829 -0.151459144 0.3635675
## 3 y3_s 0.1569910 0.1229460 1.2769107 0.20163378 -0.086291865 0.4033177
##         values
## 1   90.4430192
## 2    0.2951881
## 3 2943.2098667</code></pre>
<p>Once the joining is successful, I can divide ‚Äúestimate‚Äù, ‚Äúconf.low‚Äù and ‚Äúconf.high‚Äù by the standard deviation in ‚Äúvalues‚Äù via <code>mutate_at()</code>. I will round the results, as well, although I‚Äôm ignoring the vast differences in the variable range when I do this (this is treating the situation as if it were simpler than it really is).</p>
<pre class="r"><code>coef_st %&gt;%
     inner_join(., sd_all, by = c(&quot;term&quot; = &quot;ind&quot;) ) %&gt;%
     mutate_at( vars(estimate, conf.low, conf.high), funs(round( ./values, 4) ) )</code></pre>
<pre><code>##   term estimate std.error statistic    p.value conf.low conf.high
## 1 y1_s   0.0029 0.1395533 1.8918334 0.05851318  -0.0001    0.0061
## 2 y2_s   0.3495 0.1294722 0.7968153 0.42555829  -0.5131    1.2316
## 3 y3_s   0.0001 0.1229460 1.2769107 0.20163378   0.0000    0.0001
##         values
## 1   90.4430192
## 2    0.2951881
## 3 2943.2098667</code></pre>
<p>I‚Äôll also get rid of the extra variables I don‚Äôt want and only keep the unstandardized coefficients and confidence interval limits along with the variable name.</p>
<pre class="r"><code>coef_unst = coef_st %&gt;%
     inner_join(., sd_all, by = c(&quot;term&quot; = &quot;ind&quot;) ) %&gt;%
     mutate_at( vars(estimate, conf.low, conf.high), funs(round( ./values, 4) ) ) %&gt;%
     select(-(std.error:p.value), -values)

coef_unst</code></pre>
<pre><code>##   term estimate conf.low conf.high
## 1 y1_s   0.0029  -0.0001    0.0061
## 2 y2_s   0.3495  -0.5131    1.2316
## 3 y3_s   0.0001   0.0000    0.0001</code></pre>
</div>
<div id="estimates-from-the-unstandardized-model" class="section level2">
<h2>Estimates from the unstandardized model</h2>
<p>Note that the estimated coefficients are the same from the model using unstandardized variables and the model where we unstandardized the coefficients after fitting with standardized variables.</p>
<pre class="r"><code>round( fixef(fit1)[2:4], 4)</code></pre>
<pre><code>##     y1     y2     y3 
## 0.0029 0.3495 0.0001</code></pre>
<p>Given that the estimates are the same, couldn‚Äôt we simply go back and fit the unstandardized model and ignored the warning message? Unfortunately the convergence issues can cause problems when trying to calculate profile likelihood confidence intervals, so the simpler approach doesn‚Äôt always work.</p>
<p>In this case there are a bunch of warnings (not shown), and confidence interval limits aren‚Äôt successfully calculated for some of the coefficients.</p>
<pre class="r"><code>tidy(fit1, effects = &quot;fixed&quot;,
     conf.int = TRUE,
     conf.method = &quot;profile&quot;)</code></pre>
<pre><code>## Computing profile confidence intervals ...</code></pre>
<pre><code>##          term      estimate    std.error  statistic      p.value
## 1 (Intercept) -4.582545e+00 1.094857e+00 -4.1855190 2.845153e-05
## 2          y1  2.919191e-03 1.520407e-03  1.9200071 5.485700e-02
## 3          y2  3.495238e-01 4.370778e-01  0.7996834 4.238943e-01
## 4          y3  5.334512e-05 4.049206e-05  1.3174218 1.876973e-01
##        conf.low    conf.high
## 1            NA           NA
## 2            NA           NA
## 3            NA           NA
## 4 -2.931883e-05 0.0001370332</code></pre>
</div>
</div>
<div id="further-important-work" class="section level1">
<h1>Further (important!) work</h1>
<p>These results are all on the scale of log-odds, and I would exponentiate the unstandardized coefficients to the odds scale for reporting and interpretation. Along these same lines, one thing I didn‚Äôt discuss in this post that is important to consider is the appropriate and interesting unit increase for each variable. Clearly the effect of a ‚Äú1 unit‚Äù increase in the variable is likely not of interest for at least <code>y2</code> (range between 0 and 1) and <code>y3</code> (range between 10000 and 20000). In the first case, 1 unit encompasses the entire range of the variable and in the second case 1 unit appears to be much smaller than the scale of the measurement.</p>
<p>The code to do this would be similar to what I‚Äôve done above. I would create a data.frame with the unit increase of interest for each variable in it, join this to the coefficients dataset, and multiply all estimates and CI by those values. I‚Äôd make sure to report these in any tables of results so the reader can see that the reported estimate is a change in estimated odds/mean for some practically important increase in the variable. This multiplying step can occur before or after unstandardizing but must happen before doing exponentiation/inverse-linking.</p>
</div>
